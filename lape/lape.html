<h2 id="language-specific-neurons-finding-language-specific-neurons-with-lape-language-activation-probability-entropy-">Language-Specific Neurons: Finding language-specific neurons with LAPE (Language Activation Probability Entropy)</h2>
<h3 id="-project-description">üìù Project description</h3>
<p>This project analyzes neuron-level language specialization in multilingual language models, implementing and extending the ideas from the paper <a href="https://arxiv.org/abs/2402.16438">Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models</a>.
It enables detailed inspection of neuron activations across layers, entropy-based identification of language-specific neurons, and visualization of multilingual representations.</p>
<p>Unlike the original implementation (which uses VLLM and is limited to specific models), this project offers a more modular and extensible approach using Hugging Face Transformers and PyTorch.</p>
<p><strong>Key improvements:</strong></p>
<ul>
<li>convert the original multilingual dataset to JSON for clarity</li>
<li>records neuron activations via forward pass and <strong>pytorch hook</strong></li>
<li>computes <strong>entropy-based</strong> metrics (LAPE) to identify language-specific neurons</li>
<li>includes scripts for <strong>visualizing activations and entropy results</strong>, showing that top-layer neurons are often the most language-specific</li>
</ul>
<p>This setup makes it easier to extend the analysis to other models and deepen understanding of multilingual representations. </p>
<h3 id="-limitation">üöß Limitation</h3>
<p>The code is designed to be <strong>adaptable</strong>, but currently is tested only on a few models (see &#39;üéñÔ∏è Supported (tested) models&#39;).   </p>
<h3 id="-file-structure">üìÅ File structure</h3>
<h5 id="scripts-">Scripts:</h5>
<ol>
<li><p><strong><code>record_activations.py</code></strong>
Tokenizes the dataset, attaches PyTorch hooks to capture neuron activations, and runs a forward pass through the model. Activations above a threshold (default = 0) are saved to file. This script can be run independently or will be triggered by <code>calculate_entropy.py</code>. Dataset is a <code>.json</code> file in <code>data/</code>, containing inputs in 7 languages.</p>
</li>
<li><p><strong><code>visualize_activations.py</code></strong>
Generates a heatmap of neuron activations and displays basic info such as token count, activation shape, and number of active neurons</p>
</li>
<li><p><strong><code>calculate_entropy.py</code></strong>
Loads saved activations and computes LAPE (Language Activation Probability Entropy) to identify the most language-specific neurons</p>
</li>
<li><p><strong><code>visualize_enthropies.py</code></strong>
Plots a matrix showing the number of language-specific neurons per layer and language. Typically, reveals that the top model layers contain the most language-specific neurons</p>
</li>
</ol>
<blockquote>
<p>To better understand what each script does, read the header comments (docstring) at the top of each Python file</p>
</blockquote>
<h5 id="folders-">Folders:</h5>
<p>All folders are automatically generated by the code:</p>
<ol>
<li><code>activations/</code> 
Contains subfolders named after the models. Each subfolder stores the recorded neuron activations per language, <code>generated by record_activations.py</code></li>
<li><code>data/</code> 
Stores the main dataset in JSON format. The <code>data/raw/</code> subfolder contains the original raw data used to create the dataset. If you choose to save tokenized data, it will also be stored here for reuse</li>
<li><code>enthropies/</code>
Contains subfolders named after the models. Each includes a file listing neurons with the lowest LAPE entropy-i.e., the most language-specific neurons. These can later be used for experiments such as neuron suppression or amplification</li>
</ol>
<hr>
<h3 id="-how-to-use">‚öôÔ∏è How to use</h3>
<p>To get started, follow these steps:</p>
<ol>
<li>first, clone the repo <pre><code class="lang-bash">git <span class="hljs-keyword">clone</span> <span class="hljs-title">&lt;GitHub</span> repository url&gt;
</code></pre>
</li>
<li>install dependencies<pre><code class="lang-bash">pip <span class="hljs-keyword">install</span> -r requirements.txt
</code></pre>
</li>
</ol>
<h4 id="-running-the-pipeline">üöÄ Running the pipeline</h4>
<h6 id="you-can-either-run-each-script-individually-in-the-order-below-">You can either run each script individually (in the order below):</h6>
<ol>
<li><p><code>record_activations.py</code>
Generates neuron activation data based on input language and model.</p>
<pre><code>options:
-h, <span class="hljs-comment">--help            show this help message and exit</span>
<span class="hljs-comment">--language LANGUAGE   Language to process. Must be one of: en, es, fr, id, ja, vi, zh. Used to select the appropriate input text from 'language-texts.json'.</span>
<span class="hljs-comment">--model_name MODEL_NAME</span>
                     Model name <span class="hljs-keyword">or</span> path <span class="hljs-keyword">on</span> Hugging Face. Must support <span class="hljs-keyword">access</span> <span class="hljs-keyword">to</span> MLP/FFN layers. Tested: <span class="hljs-symbol">'openai</span>-community/gpt2', <span class="hljs-symbol">'Qwen</span>/Qwen2.<span class="hljs-number">5</span>-<span class="hljs-number">0.5</span>B-Instruct'.
-a, <span class="hljs-comment">--activation_threshold ACTIVATION_THRESHOLD</span>
                     Threshold value above which a neuron<span class="hljs-symbol">'s</span> output <span class="hljs-keyword">is</span> considered <span class="hljs-symbol">'activated</span>'. Used <span class="hljs-keyword">to</span> count activations during forward passes.
-s, <span class="hljs-comment">--save            Save the tokenized input tensors to disk (for reuse or inspection).</span>
-t, <span class="hljs-comment">--allow_truncation</span>
                     Allow truncation
-v, <span class="hljs-comment">--visualize       After processing, generate a heatmap visualization of the neuron activations. Requires 'visualize_activations.py' to be present.</span>
</code></pre><p>This is the default call (automatically used if no flags are provided):</p>
<pre><code class="lang-bash"><span class="hljs-keyword">python</span> record_activations.<span class="hljs-keyword">py</span> --<span class="hljs-keyword">language</span> <span class="hljs-keyword">en</span> --model_name openai-community/gpt2 --activation_threshold <span class="hljs-number">0</span>
</code></pre>
<blockquote>
<p>If not run beforehand, this script will be automatically executed by <code>calculate_entropy.py</code>.</p>
</blockquote>
</li>
<li><p><code>visualize_activations.py</code>:
Visualizes the activation data. This is auto-called from record_activations.py when the -v flag is set.</p>
<pre><code>option<span class="hljs-variable">s:</span>
-h, --<span class="hljs-keyword">help</span>                show <span class="hljs-keyword">help</span> message
-<span class="hljs-keyword">p</span>, --activations_path    Relative path (activations/<span class="hljs-symbol">&lt;model&gt;</span>/<span class="hljs-symbol">&lt;lang&gt;</span>-recorder-activations.<span class="hljs-keyword">pt</span>)<span class="hljs-keyword">to</span> <span class="hljs-keyword">a</span> .<span class="hljs-keyword">pt</span> <span class="hljs-keyword">file</span> containing neuron activations data <span class="hljs-keyword">to</span> <span class="hljs-keyword">be</span> visualized
</code></pre><pre><code class="lang-bash"><span class="hljs-keyword">python</span> visualize_activations.<span class="hljs-keyword">py</span> --activations_path activations/gpt2/<span class="hljs-keyword">en</span>-recorded-activations.<span class="hljs-keyword">pt</span>
</code></pre>
</li>
<li><p><code>calculate_entropy.py</code>:
Calculates neuron entropy across languages and filters language-specific neurons.</p>
<pre><code>options:
-h, <span class="hljs-comment">--help            show this help message and exit</span>
-m MODEL_NAME, <span class="hljs-comment">--model_name MODEL_NAME</span>
                     Name <span class="hljs-keyword">or</span> path <span class="hljs-keyword">of</span> the HuggingFace model used <span class="hljs-keyword">for</span> activation recording. Tested examples: <span class="hljs-symbol">'openai</span>-community/gpt2', <span class="hljs-symbol">'Qwen</span>/Qwen2.<span class="hljs-number">5</span>-<span class="hljs-number">0.5</span>B-Instruct'.
<span class="hljs-comment">--top_activation_filter_ratio TOP_ACTIVATION_FILTER_RATIO</span>
                     Ratio used <span class="hljs-keyword">to</span> filter <span class="hljs-keyword">out</span> inactive neurons. Only neurons <span class="hljs-keyword">with</span> activation <span class="hljs-keyword">in</span> the top (<span class="hljs-number">1</span> - ratio) percentile across any language are kept. <span class="hljs-keyword">Default</span>: <span class="hljs-number">0.95</span> (i.e., keep top <span class="hljs-number">5</span> percent).
-a, <span class="hljs-comment">--activation_bar_ratio ACTIVATION_BAR_RATIO</span>
                     Minimum activation threshold <span class="hljs-keyword">for</span> final neuron selection. Neurons must exceed this activation level <span class="hljs-keyword">in</span> their most responsive language <span class="hljs-keyword">to</span> be considered language-specific. <span class="hljs-keyword">Default</span>: <span class="hljs-number">0.95</span>.
-r, <span class="hljs-comment">--rerun_all_activation_for_languages</span>
                     <span class="hljs-keyword">Force</span> re-running the activation recording step <span class="hljs-keyword">for</span> <span class="hljs-keyword">all</span> languages, even <span class="hljs-keyword">if</span> cached activation files already exist.
<span class="hljs-comment">--activation_threshold ACTIVATION_THRESHOLD</span>
                     Threshold value above which a neuron<span class="hljs-symbol">'s</span> output <span class="hljs-keyword">is</span> considered <span class="hljs-symbol">'activated</span>'. Used <span class="hljs-keyword">to</span> count activations during forward passes.
-v, <span class="hljs-comment">--visualize       If set, enables visualization of recorded neuron entropies.</span>
-t, <span class="hljs-comment">--allow_truncation</span>
                     Allow truncation
-u, <span class="hljs-comment">--use_saved       Use the saved tokenized input, avoid tokenization</span>
-s, <span class="hljs-comment">--save            Save the tokenized input tensors to disk (for reuse or inspection).</span>
</code></pre><blockquote>
<p>Currently, only en and es languages are defaulted, go to <code>calculate_entropy.py</code> line 56 and change that to your needs</p>
</blockquote>
</li>
</ol>
<p>This is the default call (automatically used if no flags are provided):</p>
<pre><code class="lang-bash">python calculate_entropy.py --model_name openai-community/gpt2 --top_activation_filter_ratio <span class="hljs-number">0.95</span> --activation_bar_ratio <span class="hljs-number">0.95</span>`. This <span class="hljs-keyword">code</span> actually implements the calculation <span class="hljs-keyword">of</span> entropy and by using `-v` flag also calls the `visualize_entropies.py
</code></pre>
<blockquote>
<p>If activation data from record_activations.py is missing, calculate_entropy.py will automatically run record_activations.py for the required language.</p>
</blockquote>
<ol>
<li><code>visualize_entropies.py</code>:
Used to visualize entropy data generated in the previous step.<pre><code>option<span class="hljs-variable">s:</span>
-h, --<span class="hljs-keyword">help</span>            show this <span class="hljs-keyword">help</span> message <span class="hljs-built_in">and</span> <span class="hljs-keyword">exit</span>
-<span class="hljs-keyword">p</span>, --entropy_path ENTROPY_PATH
                     Relative path (entropies/<span class="hljs-symbol">&lt;model&gt;</span>/<span class="hljs-keyword">language</span>-specific-neurons.<span class="hljs-keyword">pt</span>) <span class="hljs-keyword">to</span> <span class="hljs-keyword">a</span> .<span class="hljs-keyword">pt</span> <span class="hljs-keyword">file</span> containing entropy data <span class="hljs-keyword">to</span> <span class="hljs-keyword">be</span> visualized
</code></pre><pre><code class="lang-bash"><span class="hljs-keyword">python</span> visualize_entropies.<span class="hljs-keyword">py</span> --entropy_path enthropies/gpt2/<span class="hljs-keyword">language</span>-specific-neurons.<span class="hljs-keyword">pt</span>
</code></pre>
<blockquote>
<p>This is automatically triggered from <code>calculate_entropy.py</code> if the -v flag is used</p>
</blockquote>
</li>
</ol>
<h6 id="or-run-full-pipeline-at-once-">Or run full pipeline at once:</h6>
<p><code>calculate_entropy.py</code> is designed to be generating everything it is missing </p>
<pre><code class="lang-bash"><span class="hljs-keyword">python</span> calculate_entropy.<span class="hljs-keyword">py</span>
</code></pre>
<hr>
<h3 id="-supress-or-amplify-neurons">üìà Supress or amplify neurons</h3>
<p>You can experiment with disabling or boosting specific neurons associated with certain languages using the notebook <code>LAPE_language_specific_neurons.ipynb</code>
To use the neuron manipulation function in your own experiments, call <code>activate_hook()</code> with the following arguments:</p>
<pre><code><span class="hljs-comment">----------------------------------------------------------------------------------------------------------------------</span>
PARAMETER       DESCRIPTION                                                 SUPORTED VALUES
<span class="hljs-comment">----------------------------------------------------------------------------------------------------------------------</span>
original_lang  | Original language that <span class="hljs-keyword">the</span> model will be prompted <span class="hljs-keyword">with</span>   | <span class="hljs-string">"en"</span>, <span class="hljs-string">"es"</span>, <span class="hljs-string">"fr"</span>, <span class="hljs-string">"id"</span>, <span class="hljs-string">"ja"</span>, <span class="hljs-string">"vi"</span>, <span class="hljs-string">"zh"</span>
efected_lang   | Language naurons that will be amplified/surpressed       | <span class="hljs-string">"en"</span>, <span class="hljs-string">"es"</span>, <span class="hljs-string">"fr"</span>, <span class="hljs-string">"id"</span>, <span class="hljs-string">"ja"</span>, <span class="hljs-string">"vi"</span>, <span class="hljs-string">"zh"</span>
mode           | Whether <span class="hljs-built_in">to</span> supress <span class="hljs-keyword">or</span> amplify selected neurons           | <span class="hljs-string">"amplify"</span>, <span class="hljs-string">"supress"</span>
factor         | Strength <span class="hljs-keyword">of</span> amplification only <span class="hljs-keyword">for</span> mode=<span class="hljs-string">"amplify"</span>        | <span class="hljs-keyword">any</span> floating <span class="hljs-built_in">number</span>, (e.g <span class="hljs-number">10.0</span>, <span class="hljs-number">100.0</span>)
colab          | <span class="hljs-built_in">set</span> `True` <span class="hljs-keyword">if</span> <span class="hljs-keyword">using</span> <span class="hljs-keyword">in</span> colab (<span class="hljs-keyword">for</span> path fetching)         | True <span class="hljs-keyword">or</span> False
<span class="hljs-comment">----------------------------------------------------------------------------------------------------------------------</span>
</code></pre><p>With the following settings:</p>
<pre><code><span class="hljs-attr">original_lang</span> = <span class="hljs-string">"ja"</span>, 
<span class="hljs-attr">effected_lang</span> = <span class="hljs-string">"en"</span>, 
<span class="hljs-attr">mode</span>          = <span class="hljs-string">"amplify"</span>, 
<span class="hljs-attr">factor</span>        = <span class="hljs-number">100.0</span>, 
<span class="hljs-attr">colab</span>         = <span class="hljs-literal">True</span>
</code></pre><p>The model, prompted in Japanese, starts with a correct Japanese phrase <em>&quot;Once upon a time,&quot;</em> but then breaks down into nonsensical English gibberish:</p>
<pre><code>„ÇÄ„Åã„Åó„ÇÄ„Åã„Åó„ÄÅ,{<span class="hljs-comment">"  Clintons disastr  suche suche (1 Clintons2 under the  deep the paramMap iht  well work iht non,www  still the manh Clintons suche1 the springfox first right√¶nd1 suche1 high the3 even</span>
</code></pre><p><strong>üî• Congratulations, you broke the Language Machine!</strong> </p>
<hr>
<h3 id="-how-to-use-with-other-models">üìå How to use with other models</h3>
<p>If you&#39;re using a model other than the ones listed under supported models, you might encounter errors. This is usually because the script cannot automatically determine the model&#39;s intermediate (feedforward) layer size.</p>
<p>To resolve this, you have two options:</p>
<ul>
<li>Manually update the script - Go to <code>record_activations.py</code> (around line 95) and add a new if case for your model&#39;s path and its intermediate_size</li>
<li>Use the <code>-i</code> flag - Directly pass the intermediate size when running the script</li>
</ul>
<h4 id="-how-to-find-the-intermediate-size">üîç How to Find the Intermediate Size</h4>
<p>To help with this, the <code>analyze_model.py</code> utility script is included. It inspects the model and prints out the number of layers and the detected intermediate_size</p>
<p>example usage:</p>
<pre><code class="lang-bash">python analyze_model<span class="hljs-selector-class">.py</span> -m microsoft/phi-<span class="hljs-number">2</span>
</code></pre>
<p>sample output:</p>
<pre><code><span class="hljs-comment">--- Model Info ---</span>
Number <span class="hljs-keyword">of</span> layers: <span class="hljs-number">32</span>
Detected intermediate_size <span class="hljs-built_in">from</span> <span class="hljs-string">'model.layers.0.self_attn.dense'</span>: <span class="hljs-number">2560</span>
Max <span class="hljs-keyword">token</span> <span class="hljs-built_in">length</span>: <span class="hljs-number">2048</span>

üîé Searching <span class="hljs-keyword">for</span> transformer block list...

‚úÖ Found possible layer containers:
 - model.model.layers (<span class="hljs-built_in">length</span>: <span class="hljs-number">32</span>)
</code></pre><blockquote>
<p>From this, we can see the model‚Äôs intermediate_size is 2560. You can now either hardcode this value into record_activations.py (around line 95), or pass it directly using the -i flag.</p>
</blockquote>
<p>Or you can read the correct size from error, for example: <code>RuntimeError: The size of tensor a (3560) must match the size of tensor b (2560) at non-singleton dimension 0</code></p>
<h4 id="-example-overriding-intermediate_size">‚úÖ Example: Overriding intermediate_size</h4>
<p>Once you‚Äôve found the correct value (either from an error message or by running <code>analyze_model.py</code>), use the <code>-i</code> flag like this:</p>
<pre><code class="lang-bash">python calculate_entropy<span class="hljs-selector-class">.py</span> -m microsoft/phi-<span class="hljs-number">2</span> -<span class="hljs-selector-tag">i</span> <span class="hljs-number">2560</span> -v
</code></pre>
<h4 id="-why-is-this-hardcoded-">‚ùì Why is this hardcoded?</h4>
<p>Unfortunately, every model architecture can be a bit different - with unique parameter names or dimensions. There&#39;s no universal way to extract intermediate_size automatically for every model, which is why some manual intervention is necessary.</p>
<hr>
<h3 id="-how-to-use-with-different-data">üìå How to use with different data</h3>
<p>By default, <code>record_activations.py</code> loads its input data from the file <code>data/language-texts.json</code>.</p>
<p>If you want to use your own data, you have two options:</p>
<ul>
<li>Overwrite the existing <code>language-texts.json</code> with your own texts in the same format</li>
<li>Modify the code in <code>calculate_entropy.py</code> to use <code>record_activations_custom.py</code> rather than the default, modify the prepared file to your need</li>
</ul>
<p>Or:
You&#39;re free to use any other method ;) and multilingual dataset to record activations from. Just keep in mind:
You‚Äôll need to tokenize your data using the appropriate tokenizer.
The only file that handles data loading is record_activations.py - so you only need to modify this one script. No changes are needed in other parts of the codebase.</p>
<h3 id="-supported-_-tested-_-models-">üéñÔ∏è Supported <em>(tested)</em> models:</h3>
<ul>
<li>GPT2 (<code>openai-community/gpt2</code>)</li>
<li>Qwen2.5-0.5B-Instruct (<code>Qwen/Qwen2.5-0.5B-Instruct</code>)</li>
<li>Qwen3-8B (<code>Qwen/Qwen3-8B</code>)</li>
<li>Qwen3-4B (<code>Qwen/Qwen3-4B</code>)</li>
<li>TinyLlama-1.1B-Chat-v1.0 (<code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code>)</li>
<li>phi-2 (<code>microsoft/phi-2</code>)</li>
</ul>
<hr>
<h3 id="-results">üìä Results</h3>
<p>Results will look like this:</p>
<h4 id="neuron-activations-">Neuron activations:</h4>
<p><img src="/lape/activations.png" alt="photo of neuron activations" width="500"/></p>
<h4 id="entropies-">Entropies:</h4>
<p><img src="/lape/ent.png" alt="photo of heatmap of neurons selected as language specific" width="500"/></p>
<blockquote>
<p>Note: 
Model-specific configurations may vary. You‚Äôll need to adjust parameter names and settings for each model through experimentation</p>
</blockquote>
<h3 id="-examples-of-insights">üí¨ Examples of Insights</h3>
<p>We observe that the top-most layers contain a disproportionately high number of language-specific neurons, indicating that language separation is mostly an upper-layer phenomenon.</p>
<h3 id="-acknowledgement-credits">üìö Acknowledgement &amp; Credits</h3>
<ul>
<li>This project is an independent re-implementation insipired by this <a href="https://arxiv.org/abs/2402.16438">paper</a><ul>
<li>While the core idea is based on their work, I entirely re-implemented the original code from scratch using Hugging Face models and APIs, as it fits our needs better</li>
</ul>
</li>
<li>This project was developed as part of my internship at KInIT (Kempelen Institute of Intelligent Technologies)<ul>
<li>The work was conducted within the scope of my paid internship, and credits for research context and institutional support belong to KInIT.</li>
<li>However, the code and implementation presented in this repository are my personal work, developed during the internship at KInIT</li>
</ul>
</li>
</ul>
